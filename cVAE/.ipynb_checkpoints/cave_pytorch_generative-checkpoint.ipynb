{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 500         # number of data points in each batch\n",
    "N_EPOCHS = 1000           # times to run the model on complete data\n",
    "INPUT_DIM = 7     # size of each input\n",
    "HIDDEN_DIM = 128        # hidden dimension\n",
    "LATENT_DIM = 3         # latent vector dimension\n",
    "N_CLASSES = 1          # number of classes in the data\n",
    "lr = 1e-3               # learning rate\n",
    "\n",
    "# transforms = transforms.Compose([transforms.ToTensor()])\n",
    "# train_dataset = datasets.MNIST(\n",
    "#     './data',\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=transforms)\n",
    "\n",
    "# test_dataset = datasets.MNIST(\n",
    "#     './data',\n",
    "#     train=False,\n",
    "#     download=True,\n",
    "#     transform=transforms\n",
    "# )\n",
    "\n",
    "\n",
    "## MoS2 - 02-18-2021 dataset\n",
    "rng = 0\n",
    "data_x = pd.read_csv('../Simulated_DataSets/MoS2/data_x.csv', header=None).values\n",
    "data_y = pd.read_csv('../Simulated_DataSets/MoS2/data_y.csv', header=None).values\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(data_x, data_y, test_size=0.2, random_state=rng)\n",
    "\n",
    "x_train = torch.tensor(xtrain, dtype=torch.float)\n",
    "y_train = torch.tensor(ytrain, dtype=torch.float)\n",
    "x_test = torch.tensor(xtest, dtype=torch.float)\n",
    "y_test = torch.tensor(ytest, dtype=torch.float)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(x_test, y_test),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(x_train, y_train),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    \n",
    "\n",
    "# train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "def idx2onehot(idx, n=N_CLASSES):\n",
    "\n",
    "    assert idx.shape[1] == 1\n",
    "    assert torch.max(idx).item() < n\n",
    "\n",
    "    onehot = torch.zeros(idx.size(0), n)\n",
    "    onehot.scatter_(1, idx.data, 1)\n",
    "\n",
    "    return onehot\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, n_classes):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            latent_dim: A integer indicating the latent size.\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim + n_classes, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim + n_classes]\n",
    "\n",
    "        hidden = F.relu(self.linear(x))\n",
    "        # hidden is of shape [batch_size, hidden_dim]\n",
    "\n",
    "        # latent parameters\n",
    "        mean = self.mu(hidden)\n",
    "        # mean is of shape [batch_size, latent_dim]\n",
    "        log_var = self.var(hidden)\n",
    "        # log_var is of shape [batch_size, latent_dim]\n",
    "\n",
    "        return mean, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, n_classes):\n",
    "        '''\n",
    "        Args:\n",
    "            latent_dim: A integer indicating the latent size.\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            output_dim: A integer indicating the size of output (in case of MNIST 28 * 28).\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim + n_classes, hidden_dim)\n",
    "        self.hidden_to_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, latent_dim + num_classes]\n",
    "        x = F.relu(self.latent_to_hidden(x))\n",
    "        # x is of shape [batch_size, hidden_dim]\n",
    "#         generated_x = F.sigmoid(self.hidden_to_out(x))\n",
    "        generated_x = self.hidden_to_out(x)\n",
    "        # x is of shape [batch_size, output_dim]\n",
    "\n",
    "        return generated_x\n",
    "\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, n_classes):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            latent_dim: A integer indicating the latent size.\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim, n_classes)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim, n_classes)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "\n",
    "        # encode\n",
    "        z_mu, z_var = self.encoder(x)\n",
    "\n",
    "        # sample from the distribution having latent parameters z_mu, z_var\n",
    "        # reparameterize\n",
    "        std = torch.exp(z_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_sample = eps.mul(std).add_(z_mu)\n",
    "\n",
    "        z = torch.cat((x_sample, y), dim=1)\n",
    "\n",
    "        # decode\n",
    "        generated_x = self.decoder(z)\n",
    "\n",
    "        return generated_x, z_mu, z_var\n",
    "\n",
    "\n",
    "def calculate_loss(x, reconstructed_x, mean, log_var):\n",
    "    # reconstruction loss\n",
    "    RCL = F.mse_loss(reconstructed_x, x, size_average=False)\n",
    "    # kl divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return RCL + KLD\n",
    "\n",
    "model = CVAE(INPUT_DIM, HIDDEN_DIM, LATENT_DIM, N_CLASSES)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # set the train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        # reshape the data into [batch_size, 784]\n",
    "        \n",
    "#         x = x.view(-1, 28 * 28)\n",
    "        x = x.to(device)\n",
    "        # convert y into one-hot encoding\n",
    "#         y = idx2onehot(y.view(-1, 1))\n",
    "        y = y.to(device)\n",
    "\n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        reconstructed_x, z_mu, z_var = model(x, y)\n",
    "\n",
    "        # loss\n",
    "        loss = calculate_loss(x, reconstructed_x, z_mu, z_var)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test():\n",
    "    # set the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # test loss for the data\n",
    "    test_loss = 0\n",
    "\n",
    "    # we don't need to track the gradients, since we are not updating the parameters during evaluation / testing\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            # reshape the data\n",
    "#             x = x.view(-1, 28 * 28)\n",
    "            x = x.to(device)\n",
    "\n",
    "            # convert y into one-hot encoding\n",
    "#             y = idx2onehot(y.view(-1, 1))\n",
    "            y = y.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            reconstructed_x, z_mu, z_var = model(x, y)\n",
    "\n",
    "            # loss\n",
    "            loss = calculate_loss(x, reconstructed_x, z_mu, z_var)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "# best_test_loss = float('inf')\n",
    "\n",
    "# for e in range(N_EPOCHS):\n",
    "\n",
    "#     train_loss = train()\n",
    "#     test_loss = test()\n",
    "\n",
    "#     train_loss /= len(x_train)\n",
    "#     test_loss /= len(x_test)\n",
    "\n",
    "#     print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Test Loss: {test_loss:.2f}')\n",
    "\n",
    "#     if best_test_loss > test_loss:\n",
    "#         best_test_loss = test_loss\n",
    "#         patience_counter = 1\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "\n",
    "#     if patience_counter > 3:\n",
    "#         break\n",
    "\n",
    "# # create a random latent vector\n",
    "# z = torch.randn(1, LATENT_DIM).to(device)\n",
    "\n",
    "# # pick randomly 1 class, for which we want to generate the data\n",
    "# y = torch.randint(0, N_CLASSES, (1, 1)).to(dtype=torch.long)\n",
    "# print(f'Generating a {y.item()}')\n",
    "\n",
    "# y = idx2onehot(y).to(device, dtype=z.dtype)\n",
    "# z = torch.cat((z, y), dim=1)\n",
    "\n",
    "# reconstructed_img = model.decoder(z)\n",
    "# # img = reconstructed_img.view(28, 28).data\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(img, cmap='gray')\n",
    "# plt.show()\n",
    "\n",
    "# z = torch.randn(1, LATENT_DIM).to(device)\n",
    "# y = torch.tensor([[0.5]]).to(device)\n",
    "# z = torch.cat((z, y), dim=1)\n",
    "# reconstructed_x = model.decoder(z)\n",
    "# print(reconstructed_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a path\n",
    "PATH = 'MoS2_cvae.pt'\n",
    "\n",
    "# Save\n",
    "# torch.save(model, PATH)\n",
    "\n",
    "# Load\n",
    "model = torch.load(PATH)\n",
    "# model.eval()\n",
    "\n",
    "y0 = 1.0\n",
    "num_z = 1000\n",
    "rev_x = np.zeros((num_z,7)) \n",
    "\n",
    "for i in range(num_z):\n",
    "    z = torch.randn(1, LATENT_DIM).to(device)\n",
    "    y = torch.tensor([[y0]]).to(device)\n",
    "    z = torch.cat((z, y), dim=1)\n",
    "    reconstructed_x = model.decoder(z)\n",
    "    rev_x[i,:] = reconstructed_x.cpu().data.numpy()\n",
    "#     print(reconstructed_x)\n",
    "\n",
    "fname = 'gen_samps_cvae.csv'\n",
    "np.savetxt(fname, rev_x, fmt='%.6f', delimiter=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
